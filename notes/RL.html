<!DOCTYPE html>
<html ontouchmove="" xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Satvik Gupta" />
  <title>Reinforcement Learning</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../styles/notes_style.css" />
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WX57KF6HY8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-WX57KF6HY8');
</script></head>
<body>
    
    <div id="navbar" onclick="">

    <div id="navbar_text">
        .satvik
    </div>
    <div id="navbar_links">
        <div class="navbar_link" id="home_link">
            <a href="/#home" target="_self">Home</a>
        </div>
        <div class="navbar_link" id="workexp_link">
            <a href="/#workexp" target="_self">
                Work Experience
            </a>
        </div>
        <div class=" navbar_link" id="projects_link">
            <a href="/#projects" target="_self">Projects</a>
        </div>
        <div class=" navbar_link" id="notes_link">
        <a href="/notes" target="_self">Notes</a>
    </div>
        <div class=" navbar_link" id="contacts_link">
            <a href="/#contact" target="_self">Contact</a>
        </div>
      
        
    </div>
</div>

<header id="title-block-header">
<h1 class="title">Reinforcement Learning</h1>
<p class="author">Satvik Gupta</p>
</header><div id="container">
<nav id="TOC" role="doc-toc"><div class="pdf_link_container"><a class="pdf_link" href="/notes/pdfs/RL.pdf">Download as PDF</a></div>
<ul>
<li><a href="#reinforcement-learning"
id="toc-reinforcement-learning">Reinforcement Learning</a>
<ul>
<li><a href="#exploration-vs-exploitation"
id="toc-exploration-vs-exploitation">Exploration vs
Exploitation</a></li>
<li><a href="#elements-of-reinforcement-learning"
id="toc-elements-of-reinforcement-learning">Elements of Reinforcement
Learning</a>
<ul>
<li><a href="#policy" id="toc-policy">Policy</a></li>
<li><a href="#reward-signal" id="toc-reward-signal">Reward
Signal</a></li>
<li><a href="#value-function" id="toc-value-function">Value
Function</a></li>
<li><a href="#model-of-environment" id="toc-model-of-environment">Model
of Environment</a></li>
</ul></li>
</ul></li>
<li><a href="#k-armed-bandit" id="toc-k-armed-bandit">k-Armed Bandit</a>
<ul>
<li><a href="#incremental-updates"
id="toc-incremental-updates">Incremental Updates</a></li>
<li><a href="#non-stationary-problems"
id="toc-non-stationary-problems">Non-stationary problems</a></li>
<li><a href="#optimal-initial-values"
id="toc-optimal-initial-values">Optimal Initial Values</a></li>
<li><a href="#section" id="toc-section"></a></li>
<li><a href="#gradient-bandit-algorithms"
id="toc-gradient-bandit-algorithms">Gradient Bandit Algorithms</a></li>
</ul></li>
</ul>
</nav><div id="data">
<h2 id="reinforcement-learning">Reinforcement Learning</h2>
<p>The main idea is that we learn by interacting with our environment.
RL tries to map situations to action. The agent’s goal is to maximise a
<em>reward</em>. We don’t tell the agent which action to take, or even
which action has higher reward (beyond an estimate). The agent must try
different actions on its own and <strong>learn.</strong></p>
<p>RL involves interaction between an active decision-making agent, and
ts environment. The agent is always trying to seek a goal (or mazximize
reward), even thought the environment is <em>uncertain.</em></p>
<h4 id="exploration-vs-exploitation">Exploration vs Exploitation</h4>
<p>Exploration is any action that lets the agent discover new features
about the environment, while exploitation means capitalizing on the
information we have already learned.</p>
<p>If the agent continues to exploit only past experiences, it is likely
to get stuck in a <strong>suboptimal policy.</strong></p>
<p>On the other hand, if it continues to explore without exploiting, it
<strong>might never find a good policy.</strong></p>
<h3 id="elements-of-reinforcement-learning">Elements of Reinforcement
Learning</h3>
<ol type="1">
<li>Policy</li>
<li>Reward Signal</li>
<li>Value Function</li>
<li>Model of the environment</li>
</ol>
<h4 id="policy">Policy</h4>
<p>Policy is how the agent behaves. It is a mapping of state to action,
i.e, it tells the agent what action to perform, based on the state of
the environment.</p>
<h4 id="reward-signal">Reward Signal</h4>
<p>Reward signal is the goal of the RL agent. Whenever the agent takes
an action, the environment gives it a <em>reward</em>. The agent’s goal
is to maximize its <strong>long term reward</strong>.</p>
<p>Reward signal is the basis through which we change the policy. If the
policy told us to do an action <em>x</em>, but that action resulted in a
low (or negative) reward, then we may change the policy not to suggest
<em>x</em> in that situation.</p>
<p>Reward may be a function of the state and the action taken.</p>
<h4 id="value-function">Value Function</h4>
<p>Reward signal gives us the immediate reward. Value function tells us
the long term reward of a particular state. The <em>value</em> of a
state is the total amount of reward an agent can accumulate in the
future, if it <strong>starts</strong> at that state.</p>
<h4 id="model-of-environment">Model of Environment</h4>
<p>Mimics the behavior of the environment. It lets the agent guess how
the environment will react to a particular action. Using the model, the
agent can predict the reward and next state, if it takes action
<em>x</em> from its current state. Models are used for planning - i.e,
the agent uses the model to consider many future situations before it
actually experiences them.</p>
<p>Models are optional - not all RL agents will use a model. Those using
a model are called <strong>model-based</strong>, and those without
models are called <strong>model-free</strong>. Model-free methods are
trial-and-error.</p>
<h2 id="k-armed-bandit">k-Armed Bandit</h2>
<ul>
<li>We repeatedly get a choice among <span
class="math inline">\(k\)</span> actions.</li>
<li>After each choice we get a reward, that depends on the action.</li>
<li>We repeat this <span class="math inline">\(time\;steps\)</span>
times.</li>
<li>We don’t know action values for certain. We may have estimates.</li>
</ul>
<p><strong>At time <span class="math inline">\(t\)</span></strong>,</p>
<ul>
<li><strong>Action selected</strong> – <span
class="math inline">\(A_t\)</span></li>
<li><strong>Reward received</strong> – <span
class="math inline">\(R_t\)</span></li>
<li><strong>Value for action <span
class="math inline">\(a\)</span></strong> – <span
class="math inline">\(q_*(a)\)</span> = expected reward given that <span
class="math inline">\(a\)</span> is selected.</li>
</ul>
<p><span class="math display">\[
q_*(a) = \mathbb{E}[R_T| A_t=a]
\]</span></p>
<ul>
<li><strong>Estimated value of action <span
class="math inline">\(a\)</span> </strong> – <span
class="math inline">\(Q_t(a)\)</span></li>
</ul>
<p>Ideally, <span class="math inline">\(Q_t(a)\)</span> should be as
close to <span class="math inline">\(q_*(a)\)</span> as possible.</p>
<p><strong>Greedy approach </strong> - always take the action with the
highest <span class="math inline">\(Q\)</span> value.</p>
<p><strong><span class="math inline">\(\epsilon\)</span>-greedy
approach</strong> - At each step, with probability <span
class="math inline">\(\epsilon\)</span> take a non-greedy action, i.e,
take the action which does NOT have the highest estimated value.</p>
<blockquote>
<p>Methods that estimate the value of actions, and then use those
estimates to choose which action to perform, are called
<strong>action-value methods</strong></p>
</blockquote>
<p>A way to estimate <span class="math inline">\(Q_t(a)\)</span> is to
average all the actual rewards received. This is the
<strong>sample-average</strong> method</p>
<figure>
<img src="../images/obs/rl_qt" alt="Estimation of Q_t" />
<figcaption aria-hidden="true">Estimation of <span
class="math inline">\(Q_t\)</span></figcaption>
</figure>
<p><span class="math inline">\(1_{predicate}\)</span> represents a
random variable that is 1 when <span
class="math inline">\(predicate\)</span> is true, and 0 when it’s
false.</p>
<p>If denominator is 0, we can define <span
class="math inline">\(Q_t(a)\)</span> as some fixed value, such as
0.</p>
<h4 id="incremental-updates">Incremental Updates</h4>
<p><span class="math display">\[
Q_n = \frac{1}{n-1}\sum_{i=1}^{n-1}R_i
\]</span></p>
<p>Instead of storing all the <span class="math inline">\(R_i\)</span>
values, we use incremental updating. The above formula can easily be
reduced to : <span class="math display">\[
Q_{n+1} = Q_n + \frac{1}{n}[R_n-Q_n]
\]</span> This is a frequently occurring format in RL. General form:
<span class="math display">\[
NewEstimate \leftarrow OldEstimate + StepSize[Target-OldEstimate]
\]</span> <span class="math inline">\(StepSize\)</span> is sometimes
denoted by <span class="math inline">\(\alpha\)</span> or <span
class="math inline">\(\alpha_t(a)\)</span></p>
<h4 id="non-stationary-problems">Non-stationary problems</h4>
<p>Rewards may change during the course of the problem. These problems
are called non-stationary. Incremental updating will not work for those.
In these cases, we attach more weight to more recent rewards than
rewards received a long time ago. For e.g, if a particular action gave
us reward of 1 during the starting of the problem, but more recently
it’s been giving us reward of 10, our estimated reward of it should lean
more towards 10.</p>
<p>We do this by fixing the value of <span
class="math inline">\(\alpha\)</span> , instead of above where it
changed according to <span class="math inline">\(n\)</span>.</p>
<p><span class="math display">\[
Q_{n+1} = Q_n + \alpha[R_n-Q_n]
\]</span></p>
<p>where <span class="math inline">\(\alpha \in(0,1]\)</span> is
constant. <span class="math inline">\(Q_{n+1}\)</span> becomes a
weighted average of all past rewards and the initial estimate <span
class="math inline">\(Q_{1}\)</span></p>
<p><span class="math display">\[Q_{n+1} = Q_n +
\alpha[R_n-Q_n]\]</span></p>
<p>Recursing the formula, we get <span class="math display">\[
Q_{n+1} = (1-\alpha)^nQ_1 + \sum_{i=1}^{n} \alpha(1-\alpha)^{n-i}R_i
\]</span> Past rewards (with lower <span
class="math inline">\(i\)</span>) are multiplied with higher powers of a
number that’s less than 1, i.e, they get less weightage.</p>
<blockquote>
<p>This is called <strong>exponential recency-weighted
average</strong></p>
</blockquote>
<h4 id="optimal-initial-values">Optimal Initial Values</h4>
<ul>
<li>Instead of setting all initial <span
class="math inline">\(Q\)</span> as 0, we can set them to an
optimistically high value. This promotes exploration.</li>
<li>In the 10-armed testbed, we chose <span
class="math inline">\(q_*(a)\)</span> values from a distribution with
mean 0 and variance 1. If we set initial <span
class="math inline">\(Q\)</span> as +5, this is very optimistic.
Whatever the agent will do, it will always get a reward less than 5.
Thus, it will try out other actions, which still have a estimated value
of 5.</li>
<li>All actions will be tried several times before the <span
class="math inline">\(Q\)</span> values converge.</li>
<li>Optimistic greedy (with <span
class="math inline">\(\epsilon=0\)</span>) can even perform better than
realistic (<span class="math inline">\(Q=0\)</span>) <span
class="math inline">\(\epsilon\)</span>-greedy.</li>
<li>Not suited to non-stationary problem, since it only promotes
exploration temporarily (during the start). If rewards keep changing, it
cannot do anything at a later stage in the problem.</li>
<li>More like a simple trick instead of a very useful method.</li>
</ul>
<h4 id="section"></h4>
<h4 id="gradient-bandit-algorithms">Gradient Bandit Algorithms</h4>
<ul>
<li>Instead of estimated values, each action is given a
<em>preference</em> <span class="math inline">\(H_t(a)\)</span>. Larger
preference means the action is more likely to be selected, but
preference cannot be interpreted in terms of reward. Only relative
preference of actions is important.</li>
</ul>
<p><span class="math display">\[
Pr\{A_t=a\}=\pi_t(a)=\frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}}
\]</span></p>
<p><span class="math inline">\(\pi_t(a)\)</span> represents the
probability of taking an action <span class="math inline">\(a\)</span>
at time <span class="math inline">\(t\)</span>. Initially, <span
class="math inline">\(H=0\)</span> (or some other constant) for all
<span class="math inline">\(a\)</span></p>
<p>On each step, after selecting <span
class="math inline">\({A_t}\)</span> and receiving reward <span
class="math inline">\(R_t\)</span>,we update action preferences by:
<span class="math display">\[
H_{t+1}(A_t)=H_t(A_t)+\alpha(R_t-\bar{R_t})(1-\pi_t(A_t))
\]</span> and <span class="math display">\[
H_{t+1}(a)=H_t(a)-\alpha(R_t-\bar{R_t})\pi_t(a)
\]</span> for all <span class="math inline">\(a \ne A_t\)</span></p>
<p><span class="math inline">\(\bar{R_t}\)</span> is the average of all
rewards received until (and including) time-step <span
class="math inline">\(t\)</span>. If the reward received in current step
is higher than the average, the preference for <span
class="math inline">\(A_t\)</span> goes up, otherwise it goes down.</p>
</div></div></body>
</html>
